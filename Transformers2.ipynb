{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-17T18:25:16.491095600Z",
     "start_time": "2023-10-17T18:25:16.070550800Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import ortho_group\n",
    "from embedding.schema import Schema\n",
    "from embedding.encoder import Encoder\n",
    "from embedding.structure import Struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "outputs": [],
   "source": [
    "schema = Schema(labels=['A1','A2', 'A3', 'next', 'a','b','c','d','e'], attributes=['A1', 'A2', 'A3', 'next'])\n",
    "\n",
    "n_emb = 1024\n",
    "encoder = Encoder(schema, dim=n_emb)\n",
    "E = encoder.token_emb"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-18T17:01:02.462109300Z",
     "start_time": "2023-10-18T17:01:00.569502300Z"
    }
   },
   "id": "c8e4405f0a8066b8"
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "outputs": [],
   "source": [
    "\n",
    "def attention_impl(seq, q, k, v, target, temp=100):\n",
    "    \"\"\"Causal attention over a sequence of vectors.\"\"\"\n",
    "\n",
    "    for i, s in enumerate(seq):\n",
    "        res = np.zeros((i,))\n",
    "        for j, ss in enumerate(seq[:i]):\n",
    "            res[j] = q[i].T @ k[j]\n",
    "            \n",
    "        res *= temp\n",
    "        w = np.exp(res)/np.sum(np.exp(res))\n",
    "        if i != 0:\n",
    "            seq[i][target] = np.sum([w[j]*v[j] for j in range(i)], axis=0)\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "\n",
    "def ff_decode(x, C=100):\n",
    "    \"\"\"Feed forward decoder layer\"\"\"\n",
    "    I = np.ones(E.shape[0])\n",
    "    cond = C*(E @ x - .5 * I)\n",
    "    return E.T@(relu(cond + I) - relu(cond))\n",
    "\n",
    "\n",
    "def ff_path_decode(x, p, C=100):\n",
    "    \"\"\"Feed forward path decoder layer\"\"\"\n",
    "    n_attr = encoder.attr_emb.shape[0]\n",
    "    I = np.ones(n_attr)\n",
    "    E_attr = E[:n_attr]\n",
    "    cond = C*(E_attr @ p - .5 * I).repeat(n_emb).reshape(n_attr, n_emb)\n",
    "    trans = np.array([encoder.attr_emb[i].T@x for i in range(n_attr)])\n",
    "\n",
    "    return x + np.sum(relu(cond + trans - x) - relu(cond), axis=0)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-18T17:01:02.476257500Z",
     "start_time": "2023-10-18T17:01:02.466109300Z"
    }
   },
   "id": "efa868f9061a891"
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"a\"\n",
      "\"b\"\n",
      "\"c\"\n",
      "\"d\"\n",
      "\"e\"\n"
     ]
    }
   ],
   "source": [
    "## Initialize seq with positional embedding\n",
    "p_dim = 16\n",
    "Z = ortho_group.rvs(p_dim)\n",
    "p = np.random.normal(size=p_dim)\n",
    "p /= np.linalg.norm(p)\n",
    "\n",
    "n_seq = 5\n",
    "seq = []\n",
    "for i in range(n_seq):\n",
    "    seq.append([p,  # position \n",
    "                np.zeros(n_emb), # vector\n",
    "                np.zeros(n_emb), # transformed vector\n",
    "                np.zeros(n_emb), # path\n",
    "                np.zeros(n_emb)  # token\n",
    "                ])\n",
    "    p = Z @ p\n",
    "\n",
    "\n",
    "## Place encoded vector in first position\n",
    "x = Struct.create(schema, ('a', {'A1': \n",
    "                                     ('b', {'A2': \n",
    "                                                ('c', {'A1': \n",
    "                                                           ('d', {'A3':'e'}),\n",
    "                                                       'A2':\n",
    "                                                           ('a', {'A1':'b'})\n",
    "                                                       }\n",
    "                                                 )}\n",
    "                                      )}\n",
    "                           ))\n",
    "v = encoder.encode(x)\n",
    "\n",
    "A_next = encoder.attr_emb[schema.attr_to_ind['next']]\n",
    "path = ['A1', 'A2', 'A1', 'A3']\n",
    "\n",
    "# Initialize with encoded vector\n",
    "seq[0][1] = v\n",
    "\n",
    "# Initialize path\n",
    "seq[0][3] = sum([np.linalg.matrix_power(A_next, i+1) @ E[schema.token_to_ind[p]] for i, p in enumerate(path)])\n",
    "\n",
    "\n",
    "def attention1(seq):\n",
    "    q = [Z.T@s[0] for s in seq]\n",
    "    k = [s[0] for s in seq]\n",
    "    v = [s[2] for s in seq]\n",
    "    attention_impl(seq, q, k, v, 1)\n",
    "\n",
    "def attention2(seq):\n",
    "    q = [Z.T@s[0] for s in seq]\n",
    "    k = [s[0] for s in seq]\n",
    "    v = [A_next.T@s[3] for s in seq]\n",
    "    attention_impl(seq, q, k, v, 3)\n",
    "\n",
    "def ff(seq):\n",
    "    \"\"\"Apply feed forward decoder to a sequence of vectors.\"\"\"\n",
    "    for s in seq:\n",
    "        s[2] = ff_path_decode(s[1], s[3])\n",
    "        s[4] = ff_decode(s[2])\n",
    "    \n",
    "## Run transformer\n",
    "for _ in range(n_seq):\n",
    "    ff(seq)\n",
    "    attention1(seq)\n",
    "    attention2(seq)\n",
    "    \n",
    "out = [encoder.decode(s[4]) for s in seq]\n",
    "for o in out:\n",
    "    print(o.to_strings()) if o else print(None)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-18T17:04:09.282663100Z",
     "start_time": "2023-10-18T17:04:09.162344200Z"
    }
   },
   "id": "c1285221605d475e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "476ac01030a961c1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
